// ‚úÖ VERS√ÉO 2.0 - Filtros aprimorados contra texto introdut√≥rio (deploy for√ßado)
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";
import { withRetry } from "../_shared/retry.ts";
import { validateDialectalFile, logValidationResult } from "../_shared/validation.ts";
import { logJobStart, logJobProgress, logJobComplete, logJobError } from "../_shared/logging.ts";
import { withInstrumentation } from "../_shared/instrumentation.ts";
import { createHealthCheck } from "../_shared/health-check.ts";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

/**
 * Remove null bytes e outros caracteres de controle problem√°ticos
 * que o PostgreSQL n√£o consegue armazenar em campos TEXT
 */
function sanitizeText(text: string | null | undefined): string | null {
  if (!text) return null;
  
  const original = text;
  const sanitized = text
    // Remove null bytes (\u0000)
    .replace(/\u0000/g, '')
    // Remove outros caracteres de controle problem√°ticos (0x00-0x1F exceto \t, \n, \r)
    .replace(/[\x00-\x08\x0B\x0C\x0E-\x1F]/g, '')
    // Remove caracteres de controle Unicode adicionais
    .replace(/[\u200B-\u200D\uFEFF]/g, '')
    .trim();
  
  // ‚úÖ Log DETALHADO com √≠ndice do caractere problem√°tico
  if (original !== sanitized) {
    const nullByteIndex = original.indexOf('\u0000');
    console.warn(`‚ö†Ô∏è Caracteres removidos no √≠ndice ${nullByteIndex}:`, {
      before: original.substring(0, 100),
      after: sanitized.substring(0, 100),
      length_before: original.length,
      length_after: sanitized.length
    });
  }
  
  return sanitized;
}

/**
 * Sanitiza recursivamente todos os campos de texto de um objeto
 */
function sanitizeObject(obj: any): any {
  if (!obj || typeof obj !== 'object') return obj;
  
  if (Array.isArray(obj)) {
    return obj.map(item => sanitizeObject(item));
  }
  
  const sanitized: any = {};
  for (const [key, value] of Object.entries(obj)) {
    if (typeof value === 'string') {
      sanitized[key] = sanitizeText(value);
    } else if (typeof value === 'object' && value !== null) {
      sanitized[key] = sanitizeObject(value);
    } else {
      sanitized[key] = value;
    }
  }
  
  return sanitized;
}

const BATCH_SIZE = 500; // ‚úÖ FASE 3: Otimizado de 200 para 500 (priorizar velocidade)
const UPDATE_FREQUENCY = 10; // Atualizar progresso a cada 10 batches
const TIMEOUT_MS = 90000; // ‚úÖ FASE 3: Padronizado para 90s

interface ProcessRequest {
  fileContent: string;
  volumeNum: string;
  tipoDicionario: string; // ‚úÖ NOVO: Identificador do tipo de dicion√°rio
  offsetInicial?: number; // ‚úÖ NOVO: Suporte a retomada
  jobId?: string; // ‚úÖ NOVO: ID do job existente (opcional)
}

function validateRequest(data: any): ProcessRequest {
  if (!data || typeof data !== 'object') {
    throw new Error('Payload inv√°lido');
  }
  
  const { fileContent, volumeNum, tipoDicionario, offsetInicial = 0, jobId } = data;
  
  if (!fileContent || typeof fileContent !== 'string') {
    throw new Error('fileContent deve ser uma string v√°lida');
  }
  
  // ‚úÖ Aumentado de 10MB para 20MB para acomodar overhead de serializa√ß√£o
  if (fileContent.length > 20_000_000) {
    throw new Error('fileContent excede tamanho m√°ximo de 20MB');
  }
  
  if (!volumeNum || !['I', 'II'].includes(volumeNum)) {
    throw new Error('volumeNum deve ser "I" ou "II"');
  }
  
  if (!tipoDicionario || typeof tipoDicionario !== 'string') {
    throw new Error('tipoDicionario deve ser uma string v√°lida');
  }
  
  return { fileContent, volumeNum, tipoDicionario, offsetInicial, jobId };
}

function normalizeWord(word: string): string {
  return word.toLowerCase().normalize("NFD").replace(/[\u0300-\u036f]/g, "").trim();
}

/**
 * ‚úÖ FASE 2: Extrai marcadores de uso estruturados
 * Remove colchetes e divide por barra ou espa√ßo
 */
function extractMarkers(text: string | null): string[] {
  if (!text) return [];
  return text.replace(/[\[\]]/g, '').split(/[\/\s]+/).filter(m => m.length > 0);
}

function inferCategorias(verbete: string, definicoes: any[], contextos: any): string[] {
  const categorias: Set<string> = new Set();
  const texto = `${verbete} ${JSON.stringify(definicoes)} ${JSON.stringify(contextos)}`.toLowerCase();
  
  if (/\b(cavalo|gado|tropeiro|pe√£o|est√¢ncia|campeiro|campo|la√ßo)\b/.test(texto)) categorias.add('lida_campeira');
  if (/\b(animal|ave|p√°ssaro|peixe|bicho)\b/.test(texto)) categorias.add('fauna');
  if (/\b(√°rvore|planta|flor|erva|mato|capim)\b/.test(texto)) categorias.add('flora');
  if (/\b(comida|prato|bebida|churrasco|mate|chimarr√£o)\b/.test(texto)) categorias.add('gastronomia');
  if (/\b(roupa|vestir|traje|bombacha|len√ßo|chap√©u|poncho)\b/.test(texto)) categorias.add('vestimenta');
  if (/\b(m√∫sica|dan√ßa|cantar|tocar|viol√£o|gaita)\b/.test(texto)) categorias.add('musica_danca');
  if (/\b(lugar|regi√£o|pampa|coxilha|arroio|banhado|v√°rzea)\b/.test(texto)) categorias.add('geografia');
  if (/\b(tradi√ß√£o|costume|festa|rodeio|quer√™ncia|ga√∫cho)\b/.test(texto)) categorias.add('cultura_tradicoes');
  
  return Array.from(categorias);
}

/**
 * ‚úÖ FASE 2: Parser melhorado com regex mais robusto
 * Suporta m√∫ltiplas varia√ß√µes de formato do Dialectal Volume II
 */
function parseVerbete(verbeteRaw: string, volumeNum: string, tipoDicionario: string): any | null {
  try {
    // ‚úÖ Sanitizar texto de entrada removendo null bytes
    const cleanText = sanitizeText(verbeteRaw) || '';
    
    // ‚úÖ NOVO: Detectar sub-verbetes (verbetes derivados dentro do mesmo bloco)
    // Ex: dentro de "ABA" pode ter "ABAETADO", "ABAETAR" como sub-verbetes
    const subVerbetePattern = /\n([A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-]{2,})\s+\((?:BRAS|PLAT|CAST|QUER|PORT)\)/g;
    const subVerbetes = [...cleanText.matchAll(subVerbetePattern)];
    
    if (subVerbetes.length > 1) {
      // M√∫ltiplos verbetes no mesmo bloco - processar apenas o primeiro
      // Os outros ser√£o processados como blocos separados pelo split principal
      const firstSubEnd = subVerbetes[1].index!;
      const cleanedText = cleanText.substring(0, firstSubEnd).trim();
      console.log(`‚ö†Ô∏è Sub-verbetes detectados: ${subVerbetes.length}. Processando apenas primeiro verbete.`);
      const normalizedText = cleanedText.replace(/\s+/g, ' ').replace(/[-‚Äì‚Äî]/g, '-');
      return parseVerbeteSingle(normalizedText, volumeNum, tipoDicionario);
    }
    
    const normalizedText = cleanText.replace(/\s+/g, ' ').replace(/[-‚Äì‚Äî]/g, '-');
    return parseVerbeteSingle(normalizedText, volumeNum, tipoDicionario);
  } catch (error: any) {
    console.error(`‚ùå Erro no parseVerbete:`, error.message);
    return null;
  }
}

/**
 * ‚úÖ Parser interno - extrai dados de um √∫nico verbete
 */
function parseVerbeteSingle(normalizedText: string, volumeNum: string, tipoDicionario: string): any | null {
  try {
    // ‚úÖ FASE 2: REGEX REFINADA - Captura expl√≠cita de marcadores entre colchetes
    // Grupo 1: Verbete
    // Grupo 2: Origem (BRAS|PLAT...)
    // Grupo 3: Classe Gramatical (antes dos colchetes)
    // Grupo 4: Marcadores (dentro dos colchetes) - OPCIONAL
    // Grupo 5: Resto da Classe Gramatical (ap√≥s colchetes) - OPCIONAL
    // Grupo 6: Defini√ß√£o (ap√≥s h√≠fen)
    const headerRegex = /^([A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-\(\)\s]+?)\s+\((BRAS|PLAT|CAST|QUER|BRAS\/PLAT|PORT)\s?\)\s+([^[\-]+?)(?:\[(.*?)\])?([^\-]*?)(?:\s+\-\s+|\n)(.+)$/s;
    const match = normalizedText.match(headerRegex);
    
    if (!match) {
      // ‚úÖ FASE 2: Fallback 1 - Formato mais simples sem marcadores
      const simpleRegex = /^([A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-\(\)\s]+?)\s+\((BRAS|PLAT|CAST|QUER|PORT)\s?\)\s+(.+)$/s;
      const simpleMatch = normalizedText.match(simpleRegex);
      
      if (simpleMatch) {
        const [_, verbete, origem, restoConteudo] = simpleMatch;
        
        // Tentar extrair POS do resto do conte√∫do
        const posMatch = restoConteudo.match(/^(S\.m\.|S\.f\.|Adj\.|V\.|Adv\.|Tr\.dir\.|Int\.)\s+(.+)$/i);
        const pos = posMatch ? posMatch[1] : null;
        const definicao = posMatch ? posMatch[2] : restoConteudo;
        
        const entry_type = verbete.trim().includes(' ') ? 'mwe' : 'word';
        
        const result = {
          verbete: verbete.trim(),
          verbete_normalizado: normalizeWord(verbete),
          tipo_dicionario: tipoDicionario, // ‚úÖ NOVO CAMPO
          origem_primaria: origem.replace(/\s/g, ''),
          classe_gramatical: pos,
          marcacao_temporal: null,
          frequencia_uso: null,
          definicoes: [{ texto: definicao.trim(), acepcao: 1 }],
          remissoes: [],
          contextos_culturais: {},
          categorias_tematicas: [],
          entry_type,
          volume_fonte: volumeNum,
          pagina_fonte: null,
          confianca_extracao: 0.92, // Parsing parcial - agora eleg√≠vel para valida√ß√£o
          validado_humanamente: false,
          variantes: [],
          sinonimos: [],
          termos_espanhol: [],
          influencia_platina: origem === 'PLAT',
          origem_regionalista: [origem]
        };
        
        return sanitizeObject(result);
      }
      
      // ‚úÖ FASE 2: Fallback 2 - Formato ainda mais simples (√∫ltima tentativa)
      console.log(`‚ö†Ô∏è Tentando √∫ltimo fallback para: ${normalizedText.substring(0, 80)}...`);
      
      const lastResortRegex = /^([A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á][A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-\(\)\s]+?)\s+(.+)$/s;
      const lastMatch = normalizedText.match(lastResortRegex);
      
      if (lastMatch) {
        const [_, verbete, conteudo] = lastMatch;
        
        const entry_type = verbete.trim().includes(' ') ? 'mwe' : 'word';
        
        const result = {
          verbete: verbete.trim(),
          verbete_normalizado: normalizeWord(verbete),
          tipo_dicionario: tipoDicionario, // ‚úÖ NOVO CAMPO
          origem_primaria: 'BRAS',
          classe_gramatical: null,
          marcacao_temporal: null,
          frequencia_uso: null,
          definicoes: [{ texto: conteudo.trim(), acepcao: 1 }],
          remissoes: [],
          contextos_culturais: {},
          categorias_tematicas: [],
          entry_type,
          volume_fonte: volumeNum,
          pagina_fonte: null,
          confianca_extracao: 0.88, // Parsing simples - pr√≥ximo ao threshold
          validado_humanamente: false,
          variantes: [],
          sinonimos: [],
          termos_espanhol: [],
          influencia_platina: false,
          origem_regionalista: ['BRAS']
        };
        
        return sanitizeObject(result);
      }
      
      console.error(`‚ùå Parse falhou completamente para: ${normalizedText.substring(0, 100)}`);
      return null;
    }
    
    // ‚úÖ FASE 2: Parse com match bem-sucedido usando nova regex
    const [_, verbete, origem, classeGramPre, marcadoresRaw, classeGramPost, restoDefinicao] = match;
    
    // Combinar partes da classe gramatical e limpar
    const classeGramatical = (classeGramPre + (classeGramPost || '')).trim().replace(/\s+/g, ' ');
    
    // ‚úÖ FASE 2: Extrair marcadores estruturados
    const marcadores = extractMarkers(marcadoresRaw);
    
    // Extrair marca√ß√£o temporal das partes da classe gramatical
    const temANT = /\bANT\b/.test(classeGramatical);
    const temDES = /\bDES\b/.test(classeGramatical);
    const marcacao_temporal = temANT && temDES ? 'ANT/DES' : (temANT ? 'ANT' : (temDES ? 'DES' : null));
    
    // Remover marca√ß√µes temporais da classe gramatical final
    const classeGram = classeGramatical.replace(/\b(ANT|DES|BRAS|PLAT)\b/g, '').trim();
    
    // Extrair frequ√™ncia de uso da defini√ß√£o
    const freqMatch = restoDefinicao.match(/\[(r\/us|m\/us|n\/d)\]/);
    
    // Separar defini√ß√µes m√∫ltiplas por '//'
    const definicoesBrutas = restoDefinicao.split('//').map((d: string) => d.trim()).filter((d: string) => d.length > 0);
    const definicoes = definicoesBrutas.map((def: string, idx: number) => ({
      texto: def.replace(/\[(r\/us|m\/us|n\/d)\]/g, '').trim(),
      acepcao: idx + 1
    }));
    
    // Extrair remiss√µes (V., Cf.)
    const remissoes: string[] = [];
    const remissoesRegex = /(?:V\.|Cf\.)\s+([A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-]+)/g;
    let remMatch;
    while ((remMatch = remissoesRegex.exec(restoDefinicao)) !== null) remissoes.push(remMatch[1].trim());
    
    const contextos_culturais = { autores_citados: [], regioes_mencionadas: [], notas: [] };
    const categorias = inferCategorias(verbete, definicoes, contextos_culturais);
    const entry_type = verbete.trim().includes(' ') ? 'mwe' : 'word';
    
    const result = {
      verbete: verbete.trim(),
      verbete_normalizado: normalizeWord(verbete),
      tipo_dicionario: tipoDicionario, // ‚úÖ NOVO CAMPO
      origem_primaria: origem,
      classe_gramatical: classeGram || null,
      marcacao_temporal,
      frequencia_uso: freqMatch ? freqMatch[1] : null,
      marcadores_uso: marcadores.length > 0 ? marcadores : null, // ‚úÖ FASE 2: NOVO CAMPO
      definicoes,
      remissoes: remissoes.length > 0 ? remissoes : null,
      contextos_culturais,
      categorias_tematicas: categorias.length > 0 ? categorias : null,
      entry_type,
      volume_fonte: volumeNum,
      pagina_fonte: null,
      confianca_extracao: 0.99, // ‚úÖ FASE 2: Confian√ßa aumentada com regex refinada
      validado_humanamente: false,
      variantes: [],
      sinonimos: [],
      termos_espanhol: [],
      influencia_platina: origem === 'PLAT'|| origem === 'BRAS/PLAT',
      origem_regionalista: [origem]
    };
    
    console.log(`‚úÖ Verbete parseado: ${result.verbete} (Volume ${volumeNum}) - Marcadores: [${marcadores.join(', ')}]`);
    return sanitizeObject(result);
    
  } catch (error: any) {
    console.error(`‚ùå Erro cr√≠tico ao parsear verbete: ${error.message}`);
    return null;
  }
}

async function processInBackground(jobId: string, verbetes: string[], volumeNum: string, tipoDicionario: string, offsetInicial: number) {
  const MAX_PROCESSING_TIME = 30 * 60 * 1000;
  const timeoutPromise = new Promise((_, reject) => 
    setTimeout(() => reject(new Error('Timeout: 30 minutos excedidos')), MAX_PROCESSING_TIME)
  );

  try {
    await Promise.race([
      processVerbetesInternal(jobId, verbetes, volumeNum, tipoDicionario, offsetInicial),
      timeoutPromise
    ]);
  } catch (error: any) {
    console.error(`[JOB ${jobId}] Erro fatal:`, error);
    const supabase = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    );
    
    await supabase
      .from('dictionary_import_jobs')
      .update({
        status: 'erro',
        erro_mensagem: error.message,
        tempo_fim: new Date().toISOString()
      })
      .eq('id', jobId);
  }
}

/**
 * ‚úÖ FASE 3 - BLOCO 1: Detectar cancelamento de job
 */
async function checkCancellation(jobId: string, supabaseClient: any) {
  const { data: job } = await supabaseClient
    .from('dictionary_import_jobs')
    .select('is_cancelling')
    .eq('id', jobId)
    .single();

  if (job?.is_cancelling) {
    console.log('üõë Cancelamento detectado! Interrompendo processamento...');
    
    await supabaseClient
      .from('dictionary_import_jobs')
      .update({
        status: 'cancelado',
        cancelled_at: new Date().toISOString(),
        tempo_fim: new Date().toISOString(),
        erro_mensagem: 'Job cancelado pelo usu√°rio'
      })
      .eq('id', jobId);

    throw new Error('JOB_CANCELLED');
  }
}

async function processVerbetesInternal(jobId: string, verbetes: string[], volumeNum: string, tipoDicionario: string, offsetInicial: number) {
  const supabase = createClient(
    Deno.env.get('SUPABASE_URL') ?? '',
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
  );

  const startTime = Date.now();
  let processados = offsetInicial;
  let inseridos = 0;
  let erros = 0;
  let batchCount = 0;

  logJobStart({
    fonte: `Dialectal Vol.${volumeNum}`,
    jobId,
    totalEntries: verbetes.length,
    batchSize: BATCH_SIZE,
    timeoutMs: TIMEOUT_MS,
    maxRetries: 3
  });

  for (let i = offsetInicial; i < verbetes.length; i += BATCH_SIZE) {
    const batch = verbetes.slice(i, i + BATCH_SIZE);
    const parsedBatch: any[] = [];

    // ‚úÖ CONTADORES DE ERROS POR TIPO
    let parseErrors = {
      total: 0,
      regex_failed: 0,
      too_short: 0,
      null_bytes: 0
    };

    for (const verbeteRaw of batch) {
      const parsed = parseVerbete(verbeteRaw, volumeNum, tipoDicionario);
      if (!parsed) {
        erros++;
        parseErrors.total++;
        
        // Diagn√≥stico do tipo de erro
        if (verbeteRaw.length < 20) parseErrors.too_short++;
        else if (verbeteRaw.includes('\u0000')) parseErrors.null_bytes++;
        else parseErrors.regex_failed++;
        
        // Log de amostra (primeiros 10 ou 5% dos erros)
        if (parseErrors.total <= 10 || Math.random() < 0.05) {
          console.error(`‚ùå Parse falhou (amostra): "${verbeteRaw.substring(0, 80)}..."`);
        }
        
        continue;
      }
      
      // ‚úÖ Verbete parseado com sucesso - apenas log peri√≥dico
      if (parsedBatch.length % 100 === 0 && parsedBatch.length > 0) {
        console.log(`‚úÖ ${parsedBatch.length} verbetes parseados no batch atual`);
      }
      
      
      // ‚úÖ VALIDATION: Double-check volume_fonte e tipo_dicionario
      if (!parsed.volume_fonte) {
        console.error(`‚ö†Ô∏è CRITICAL: volume_fonte missing, setting to ${volumeNum}`);
        parsed.volume_fonte = volumeNum;
      }
      if (!parsed.tipo_dicionario) {
        console.error(`‚ö†Ô∏è CRITICAL: tipo_dicionario missing, setting to ${tipoDicionario}`);
        parsed.tipo_dicionario = tipoDicionario;
      }
      
      // ‚úÖ Sanitiza√ß√£o adicional como double-check de seguran√ßa
      parsedBatch.push(sanitizeObject(parsed));
    }

    // ‚úÖ LOG AGREGADO DE ERROS DE PARSING AO FINAL DO BATCH
    if (parseErrors.total > 0) {
      console.log(`\n‚ö†Ô∏è ERROS DE PARSING NO BATCH:`);
      console.log(`   - Total: ${parseErrors.total}`);
      console.log(`   - Regex falhou: ${parseErrors.regex_failed}`);
      console.log(`   - Muito curto: ${parseErrors.too_short}`);
      console.log(`   - Null bytes: ${parseErrors.null_bytes}`);
    }

    if (parsedBatch.length > 0) {
      // ‚úÖ VALIDA√á√ÉO: Detectar null bytes antes do upsert
      const batchJSON = JSON.stringify(parsedBatch);
      if (batchJSON.includes('\\u0000')) {
        console.error('üö® CRITICAL: Null bytes ainda presentes no batch!');
        console.error('Primeiro item com problema:', 
          parsedBatch.find(item => JSON.stringify(item).includes('\\u0000'))
        );
        throw new Error('Null bytes detectados no batch sanitizado');
      }
      
      await withRetry(async () => {
        const { error: insertError } = await supabase
          .from('dialectal_lexicon')
          .upsert(parsedBatch, { onConflict: 'verbete_normalizado,volume_fonte', ignoreDuplicates: true });
        
        if (insertError) {
          console.error(`[JOB ${jobId}] ‚ùå Erro ao inserir batch:`, insertError);
          throw insertError;
        }
      }, 3, 2000, 2);

      inseridos += parsedBatch.length;
      console.log(`[JOB ${jobId}] ‚úÖ Batch de ${parsedBatch.length} verbetes inserido com sucesso`);
    }

    processados += batch.length;
    batchCount++;

    // ‚úÖ FASE 3 - BLOCO 1: Verificar cancelamento a cada 5 batches
    // ‚úÖ OTIMIZADO: Atualizar progresso a cada 5 batches (reduz escritas)
    if (batchCount % 5 === 0 || processados >= verbetes.length) {
      // Checar se job foi cancelado
      await checkCancellation(jobId, supabase);
      
      const progressPercent = Math.round((processados / verbetes.length) * 100);
      
      await withRetry(async () => {
        const { error } = await supabase
          .from('dictionary_import_jobs')
          .update({
            verbetes_processados: processados,
            verbetes_inseridos: inseridos,
            erros: erros,
            progresso: progressPercent,
            atualizado_em: new Date().toISOString()
          })
          .eq('id', jobId);
        
        if (error) throw error;
      }, 2, 1000, 1);

      logJobProgress({
        jobId,
        processed: processados,
        totalEntries: verbetes.length,
        inserted: inseridos,
        errors: erros,
        startTime
      });
    }
  }

  const totalTime = Date.now() - startTime;
  
  await supabase
    .from('dictionary_import_jobs')
    .update({
      status: 'concluido',
      verbetes_processados: processados,
      verbetes_inseridos: inseridos,
      erros: erros,
      progresso: 100,
      tempo_fim: new Date().toISOString()
    })
    .eq('id', jobId);

  logJobComplete({
    fonte: `Dialectal Vol.${volumeNum}`,
    jobId,
    processed: processados,
    totalEntries: verbetes.length,
    inserted: inseridos,
    errors: erros,
    totalTime
  });
}

serve(withInstrumentation('process-dialectal-dictionary', async (req) => {
  // Health check endpoint
  if (req.method === 'GET' && new URL(req.url).pathname.endsWith('/health')) {
    const health = await createHealthCheck('process-dialectal-dictionary', '1.0.0');
    return new Response(JSON.stringify(health), {
      status: health.status === 'healthy' ? 200 : 503,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }

  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const rawBody = await req.json();
    const { fileContent, volumeNum, tipoDicionario, offsetInicial = 0, jobId } = validateRequest(rawBody);

    // ‚úÖ FASE 3 - BLOCO 2: Valida√ß√£o pr√©-importa√ß√£o
    const validation = validateDialectalFile(fileContent, volumeNum);
    logValidationResult(`Dialectal Vol.${volumeNum}`, validation);
    
    if (!validation.valid) {
      return new Response(
        JSON.stringify({ 
          error: 'Valida√ß√£o falhou', 
          details: validation.errors,
          warnings: validation.warnings,
          metadata: validation.metadata
        }),
        {
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
          status: 400,
        }
      );
    }

    // ‚úÖ LOG CONSOLIDADO DE ARQUIVO
    const totalLinhas = (fileContent.match(/\n/g) || []).length + 1;
    console.log(`üìä ARQUIVO RECEBIDO:`);
    console.log(`   - Volume: ${volumeNum}`);
    console.log(`   - Tamanho: ${fileContent.length.toLocaleString()} caracteres`);
    console.log(`   - Linhas totais: ${totalLinhas.toLocaleString()}`);
    console.log(`   - Offset inicial: ${offsetInicial}`);

    // üîç DEBUG: An√°lise da estrutura do arquivo
    console.log(`\nüîç DEBUG - Primeiros 500 caracteres:`);
    console.log(fileContent.substring(0, 500));
    console.log(`üîç DEBUG - Estrutura de quebras:`);
    console.log(`   - Cont√©m \\n\\n: ${fileContent.includes('\n\n')}`);
    console.log(`   - Cont√©m \\r\\n: ${fileContent.includes('\r\n')}`);
    console.log(`   - Total de \\n: ${(fileContent.match(/\n/g) || []).length}`);
    console.log(`   - Total de caracteres: ${fileContent.length}`);

    // ‚úÖ ESTRAT√âGIA CORRETA: Split com captura e reconstru√ß√£o (port do Python)
    // Passo 1: Normalizar line breaks (Windows ‚Üí Unix)
    const normalizedContent = fileContent.replace(/\r\n/g, '\n').replace(/\r/g, '\n');

    // Passo 2: Split capturando o delimitador (SEM lookahead)
    // Regex captura: "palavra MAI√öSCULA + marcador de origem"
    const verbeteDelimiter = /\n([A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-]{2,}\s+\((?:BRAS|PLAT|CAST|QUER|PORT|BRAS\/PLAT|PLAT\/CAST)\))/g;
    const parts = normalizedContent.split(verbeteDelimiter);

    console.log(`üîç Split resultou em ${parts.length} partes (esperado: √≠mpar)`);

    let allBlocks: string[] = [];

    if (parts.length > 3) {
      // Split funcionou - partes est√£o intercaladas: [conte√∫do antes, cabe√ßalho1, corpo1, cabe√ßalho2, corpo2, ...]
      
      // Parte 0: conte√∫do antes do primeiro verbete (introdu√ß√£o/lixo)
      const intro = parts[0].trim();
      if (intro.length > 100) {
        console.log(`üìù Ignorando ${intro.length} caracteres de introdu√ß√£o`);
      }
      
      // Partes √≠mpares: cabe√ßalhos de verbetes
      // Partes pares (exceto 0): corpos de verbetes
      // Reconstruir: juntar cabe√ßalho + corpo
      for (let i = 1; i < parts.length; i += 2) {
        if (i + 1 < parts.length) {
          const verbete = (parts[i] + parts[i + 1]).trim();
          if (verbete.length > 0) {
            allBlocks.push(verbete);
          }
        } else {
          // √öltimo elemento (cabe√ßalho sem corpo) - adicionar sozinho
          const verbete = parts[i].trim();
          if (verbete.length > 0) {
            allBlocks.push(verbete);
          }
        }
      }
      
      console.log(`‚úÖ Split por padr√£o de verbete: ${allBlocks.length} blocos`);
    } else {
      // FALLBACK: Split n√£o funcionou - tentar por par√°grafos
      console.warn(`‚ö†Ô∏è Split por padr√£o falhou (apenas ${parts.length} partes). Usando fallback por par√°grafos.`);
      allBlocks = normalizedContent.split(/\n{2,}/).map(v => v.trim()).filter(v => v.length > 0);
      console.log(`‚ö†Ô∏è Fallback gerou ${allBlocks.length} blocos`);
    }

    // ‚úÖ ESTAT√çSTICAS DE SPLIT
    console.log(`\nüìä ESTAT√çSTICAS DE SPLIT:`);
    console.log(`   - Total de blocos brutos: ${allBlocks.length}`);
    console.log(`   - M√©todo usado: ${parts.length > 3 ? 'Regex de verbete' : 'Fallback por par√°grafos'}`);
    console.log(`   - Blocos com < 20 chars: ${allBlocks.filter(b => b.length < 20).length}`);
    console.log(`   - Blocos com > 1000 chars: ${allBlocks.filter(b => b.length > 1000).length}`);

    // Log dos primeiros 3 blocos para valida√ß√£o manual
    console.log(`üìã Primeiros 3 blocos ap√≥s split:`);
    allBlocks.slice(0, 3).forEach((bloco, i) => {
      const primeiraLinha = bloco.split('\n')[0];
      console.log(`   ${i + 1}. ${primeiraLinha.substring(0, 80)}...`);
    });

    // ‚úÖ LOGS DE REJEI√á√ÉO AMOSTRADOS COM ESTAT√çSTICAS COMPLETAS
    const rejeitados: { 
      index: number; 
      razao: string; 
      preview: string;
      posicaoRelativa: 'inicio' | 'meio' | 'fim';
    }[] = [];

    const verbetes = allBlocks.filter((v, index) => {
      const posicaoRelativa = 
        index < allBlocks.length * 0.33 ? 'inicio' :
        index < allBlocks.length * 0.66 ? 'meio' : 'fim';
        
      // Filtro 1: Muito curto
      if (v.length < 20) {
        // Amostragem: primeiros 10 ou 5% de chance
        if (rejeitados.length < 10 || Math.random() < 0.05) {
          rejeitados.push({ index, razao: 'muito curto', preview: v, posicaoRelativa });
        }
        return false;
      }
      
      // Filtro 2: Padr√£o n√£o encontrado
      const verbetePattern = /^[A-Z√Å√Ä√É√â√ä√ç√ì√î√ö√á\-]{2,}\s+\((?:BRAS|PLAT|CAST|QUER|PORT|BRAS\/PLAT|PLAT\/CAST)\)/;
      if (!verbetePattern.test(v)) {
        if (rejeitados.length < 10 || Math.random() < 0.05) {
          rejeitados.push({ index, razao: 'padr√£o n√£o encontrado', preview: v.substring(0, 60), posicaoRelativa });
        }
        return false;
      }
      
      // Filtro 3: Se√ß√µes introdut√≥rias
      if (/^(Pref√°cio|Metodologia|Introdu√ß√£o|PATROC√çNIO|PRODU√á√ÉO|FINANCIAMENTO|SUM√ÅRIO|√çNDICE)/i.test(v)) {
        if (rejeitados.length < 10 || Math.random() < 0.05) {
          rejeitados.push({ index, razao: 'se√ß√£o introdut√≥ria', preview: v.substring(0, 60), posicaoRelativa });
        }
        return false;
      }
      
      return true;
    });

    // Estat√≠sticas agregadas de rejei√ß√µes
    const rejeicoesPorRazao = rejeitados.reduce((acc, r) => {
      acc[r.razao] = (acc[r.razao] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);

    console.log(`\nüìä ESTAT√çSTICAS DE REJEI√á√ÉO:`);
    console.log(`   - Total de blocos: ${allBlocks.length}`);
    console.log(`   - Verbetes aceitos: ${verbetes.length} (${((verbetes.length / allBlocks.length) * 100).toFixed(1)}%)`);
    console.log(`   - Blocos rejeitados: ${allBlocks.length - verbetes.length}`);
    console.log(`\n   Rejei√ß√µes por raz√£o:`);
    Object.entries(rejeicoesPorRazao).forEach(([razao, count]) => {
      console.log(`     - ${razao}: ${count}`);
    });

    // Amostra de rejeitados por posi√ß√£o no arquivo
    console.log(`\n‚ùå AMOSTRA DE BLOCOS REJEITADOS (m√°x 20):`);
    const amostrasRejeitados = rejeitados.slice(0, 20);
    amostrasRejeitados.forEach(({ index, razao, preview, posicaoRelativa }) => {
      console.log(`   [${posicaoRelativa.toUpperCase()}] Bloco ${index}: [${razao}]`);
      console.log(`      "${preview}..."`);
    });

    // Log dos aceitos
    console.log(`\n‚úÖ Primeiros 5 verbetes ACEITOS:`);
    verbetes.slice(0, 5).forEach((v, i) => {
      const primeiraLinha = v.split('\n')[0];
      console.log(`   ${i + 1}. ${primeiraLinha}`);
    });

    // ‚úÖ FASE 3: Estat√≠sticas de pr√©-processamento
    const filteredOut = allBlocks.length - verbetes.length;
    const filterRate = ((filteredOut / allBlocks.length) * 100).toFixed(1);
    
    console.log(`üìä Estat√≠sticas de pr√©-processamento:`);
    console.log(`  - Total de blocos: ${allBlocks.length}`);
    console.log(`  - Verbetes v√°lidos: ${verbetes.length}`);
    console.log(`  - Blocos filtrados: ${filteredOut}`);
    console.log(`  - Taxa de filtro: ${filterRate}%`);

    const supabase = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    );

    let jobData;

    // ‚úÖ Se jobId foi passado, usar job existente
    if (jobId) {
      const { data: existingJob, error: fetchError } = await supabase
        .from('dictionary_import_jobs')
        .select('*')
        .eq('id', jobId)
        .single();
      
      if (fetchError || !existingJob) {
        throw new Error(`Job ${jobId} n√£o encontrado: ${fetchError?.message}`);
      }
      
      jobData = existingJob;
      
      console.log(`[JOB ${jobId}] Usando job existente - tipo: ${existingJob.tipo_dicionario}`);
      
      // Atualizar job com informa√ß√µes corretas
      const { error: updateError } = await supabase
        .from('dictionary_import_jobs')
        .update({
          tipo_dicionario: tipoDicionario, // ‚úÖ Usar o tipo correto passado
          status: 'processando',
          total_verbetes: verbetes.length,
          verbetes_processados: offsetInicial,
          offset_inicial: offsetInicial,
          tempo_inicio: new Date().toISOString(),
          metadata: {
            ...(existingJob.metadata || {}),
            volume: volumeNum,
            offset: offsetInicial
          }
        })
        .eq('id', jobId);
      
      if (updateError) {
        console.error(`‚ùå Erro ao atualizar job: ${updateError.message}`);
        throw updateError;
      }
      
      console.log(`[JOB ${jobId}] Atualizado com tipo_dicionario: ${tipoDicionario}`);
    } else {
      // ‚úÖ Apenas criar novo job se jobId N√ÉO foi passado
      const { data: newJob, error: jobError } = await supabase
        .from('dictionary_import_jobs')
        .insert({
          tipo_dicionario: tipoDicionario, // ‚úÖ Usar par√¢metro correto
          status: 'iniciado',
          total_verbetes: verbetes.length,
          verbetes_processados: offsetInicial,
          offset_inicial: offsetInicial,
          tempo_inicio: new Date().toISOString(),
          metadata: { volume: volumeNum, offset: offsetInicial }
        })
        .select()
        .single();

      if (jobError) throw jobError;
      
      jobData = newJob;
      console.log(`[JOB ${newJob.id}] Criado novo job - tipo: ${tipoDicionario}`);
    }

    processInBackground(jobData.id, verbetes, volumeNum, tipoDicionario, offsetInicial);

    return new Response(
      JSON.stringify({ 
        jobId: jobData.id, 
        message: 'Processamento iniciado em background',
        totalVerbetes: verbetes.length,
        offsetInicial
      }),
      { headers: { ...corsHeaders, "Content-Type": "application/json" } }
    );

  } catch (error: any) {
    console.error('Erro ao processar requisi√ß√£o:', error);
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 400, headers: { ...corsHeaders, "Content-Type": "application/json" } }
    );
  }
}));
