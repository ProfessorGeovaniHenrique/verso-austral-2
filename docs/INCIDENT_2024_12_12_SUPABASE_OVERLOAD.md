# ðŸ“‹ Post-Mortem: Sobrecarga Supabase 2024-12-12

## Resumo Executivo

| Campo | Valor |
|-------|-------|
| **Data** | 12 de Dezembro de 2024 |
| **DuraÃ§Ã£o** | ~2-3 horas |
| **Impacto** | Sistema completamente inacessÃ­vel |
| **Severidade** | P0 - CrÃ­tico |
| **Status** | RecuperaÃ§Ã£o em andamento |

---

## Timeline

| Hora (aprox.) | Evento |
|---------------|--------|
| 14:00 | UsuÃ¡rio inicia ~50 jobs de anotaÃ§Ã£o semÃ¢ntica simultaneamente |
| 14:05 | Primeiros timeouts detectados em queries |
| 14:10 | Kill Switch acionado manualmente |
| 14:15 | Redis respondendo normalmente, DB completamente inacessÃ­vel |
| 14:20 | Tentativas de cancelar jobs via SQL falham (timeout) |
| 14:30 | Timeout persistente em todas queries SELECT simples |
| 15:00 | DocumentaÃ§Ã£o de sistema de backpressure aprimorado iniciada |
| 15:30 | ImplementaÃ§Ã£o do Job Slot Manager e thresholds progressivos |
| 16:00 | Aguardando auto-recovery do Supabase |

---

## Causa Raiz

### Causa Imediata
Excesso de jobs de anotaÃ§Ã£o semÃ¢ntica iniciados simultaneamente (~50), cada um abrindo mÃºltiplas conexÃµes ao banco de dados, excedendo o pool de conexÃµes disponÃ­vel.

### Causa Subjacente

1. **AusÃªncia de limite de concorrÃªncia global**
   - Sistema permitia iniciar nÃºmero ilimitado de jobs
   - Nenhum controle sobre quantos jobs podiam executar simultaneamente

2. **Sem pre-flight check**
   - UI nÃ£o verificava saÃºde do sistema antes de permitir novos jobs
   - BotÃ£o "Iniciar AnotaÃ§Ã£o" sempre disponÃ­vel

3. **DetecÃ§Ã£o reativa, nÃ£o proativa**
   - Sistema sÃ³ detectava sobrecarga apÃ³s jÃ¡ estar instalada
   - Cooldowns curtos (5 min) insuficientes para recuperaÃ§Ã£o

### Fatores Contribuintes

| Fator | Impacto |
|-------|---------|
| Cada job abre 3-5 conexÃµes | Multiplicador de conexÃµes |
| Edge Functions com 4min timeout | ConexÃµes mantidas abertas |
| Auto-invocation em cascata | Novas conexÃµes criadas continuamente |
| Pool Supabase ~100 conexÃµes | Limite rapidamente atingido |

### CÃ¡lculo do Impacto

```
50 jobs Ã— 4 conexÃµes/job = 200 conexÃµes simultÃ¢neas
Pool disponÃ­vel: ~100 conexÃµes
Excesso: 100+ conexÃµes aguardando = TIMEOUT
```

---

## Impacto

| MÃ©trica | Valor |
|---------|-------|
| Tempo de indisponibilidade | ~2-3h |
| UsuÃ¡rios afetados | Todos (sistema Ãºnico) |
| Dados perdidos | âœ… Nenhum (Redis preservou estado) |
| Jobs afetados | ~50 semantic + ~3 corpus + ~2 enrichment |
| Funcionalidades indisponÃ­veis | Todas que dependem de DB |

### Funcionalidades Afetadas
- âŒ Login/autenticaÃ§Ã£o
- âŒ AnotaÃ§Ã£o semÃ¢ntica
- âŒ Enriquecimento de metadados
- âŒ CatÃ¡logo de mÃºsicas
- âŒ Dashboard de anÃ¡lise
- âœ… Kill Switch (Redis independente)
- âœ… Backpressure detection (Redis independente)

---

## O Que Funcionou

| Sistema | Resultado |
|---------|-----------|
| âœ… Kill Switch via Redis | Ativou flag mesmo com DB inacessÃ­vel |
| âœ… Edge Functions respeitaram kill flag | Pararam de tentar executar |
| âœ… Mensagens de erro claras | Sistema reportou timeout corretamente |
| âœ… Cooldown persistente | Mantido via Redis TTL |
| âœ… DocumentaÃ§Ã£o existente | Facilitou diagnÃ³stico |

---

## O Que Falhou

| Sistema | Problema | Impacto |
|---------|----------|---------|
| âŒ Limite de concorrÃªncia | Inexistente | Permitiu 50+ jobs |
| âŒ Pre-flight check | Inexistente | UI nÃ£o verificou saÃºde |
| âŒ Cancel via DB | Timeout | NÃ£o conseguiu parar jobs |
| âŒ Recovery automÃ¡tico | NÃ£o ocorreu | Sistema ficou down |
| âŒ Acesso SQL direto | Bloqueado | NÃ£o pÃ´de executar limpeza |
| âŒ Cooldowns | Muito curtos | 5min insuficiente |

---

## AÃ§Ãµes Corretivas

### Imediatas (Implementadas em Sprint BP-2)

| AÃ§Ã£o | Status | Arquivo |
|------|--------|---------|
| Job Slot Manager | âœ… Feito | `job-slot-manager.ts` |
| Limita 5 jobs concorrentes via Redis | âœ… Feito | - |
| Thresholds progressivos (NORMAL/ELEVATED/HIGH/CRITICAL) | âœ… Feito | `backpressure.ts` |
| Pre-flight check `checkCanStartJob()` | âœ… Feito | `useBackpressureStatus.ts` |
| Cooldowns escalonados (3/10/30 min) | âœ… Feito | `backpressure.ts` |
| BackpressureAlert melhorado | âœ… Feito | `BackpressureAlert.tsx` |
| Contador de jobs ativos no UI | âœ… Feito | `BackpressureAlert.tsx` |
| DocumentaÃ§Ã£o de emergÃªncia | âœ… Feito | `EMERGENCY_RUNBOOK.md` |

### PrÃ³ximas (Pendente DB Recovery)

| AÃ§Ã£o | Prioridade | DependÃªncia |
|------|------------|-------------|
| Integrar `acquireJobSlot()` em `annotate-artist-songs` | P0 | DB online |
| Integrar `acquireJobSlot()` em `enrich-songs-batch` | P0 | DB online |
| Integrar `acquireJobSlot()` em `annotate-corpus` | P0 | DB online |
| Dashboard de monitoramento de conexÃµes | P1 | DB online |
| Alertas proativos via Sentry | P2 | - |
| Auto-scaling de instÃ¢ncia | P3 | AvaliaÃ§Ã£o |

---

## LiÃ§Ãµes Aprendidas

### 1. Limites sÃ£o proteÃ§Ã£o, nÃ£o obstÃ¡culo

> "5 jobs simultÃ¢neos podem parecer pouco, mas Ã© infinitamente melhor que sistema travado por 3 horas"

**AÃ§Ã£o:** Implementado Job Slot Manager com limite rÃ­gido de 5 jobs

### 2. Redis como backup crÃ­tico

> "Redis funcionou quando DB falhou. Sempre ter fallback independente para operaÃ§Ãµes crÃ­ticas."

**AÃ§Ã£o:** Kill Switch e Backpressure dependem apenas de Redis

### 3. Pre-flight checks sÃ£o essenciais

> "Verificar ANTES Ã© 1000x melhor que recuperar DEPOIS"

**AÃ§Ã£o:** Implementado `checkCanStartJob()` obrigatÃ³rio

### 4. Cooldowns devem ser proporcionais

> "5 minutos nÃ£o Ã© suficiente para recuperar de sobrecarga severa"

**AÃ§Ã£o:** Cooldowns escalonados: 3min (leve) â†’ 10min (mÃ©dio) â†’ 30min (crÃ­tico)

### 5. Monitoramento contÃ­nuo Ã© essencial

> "Se nÃ£o consegue ver, nÃ£o consegue consertar"

**AÃ§Ã£o:** Contador de jobs ativos visÃ­vel no UI, thresholds claros

### 6. DocumentaÃ§Ã£o salva vidas

> "Runbook pronto antes da prÃ³xima crise"

**AÃ§Ã£o:** `EMERGENCY_RUNBOOK.md` criado com procedimentos detalhados

---

## MÃ©tricas de Sucesso (PÃ³s-CorreÃ§Ãµes)

| MÃ©trica | Antes | Depois | Meta |
|---------|-------|--------|------|
| Jobs simultÃ¢neos mÃ¡x | âˆž (ilimitado) | 5 | â‰¤5 |
| Tempo detecÃ§Ã£o sobrecarga | ~5min | ~30s | <1min |
| Pre-flight check | âŒ NÃ£o | âœ… Sim | 100% |
| Cooldown crÃ­tico | 5min | 30min | â‰¥30min |
| Visibilidade jobs ativos | âŒ NÃ£o | âœ… Sim | 100% |
| Recovery automÃ¡tico | âŒ NÃ£o | âœ… Sim | >95% |

---

## PrevenÃ§Ã£o de RecorrÃªncia

### Controles Implementados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NOVA ARQUITETURA                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  [UsuÃ¡rio] â”€â”€â–º [Pre-flight Check] â”€â”€â–º [Job Slot Manager]   â”‚
â”‚                      â”‚                       â”‚              â”‚
â”‚                      â–¼                       â–¼              â”‚
â”‚              canStartNewJob?          acquireJobSlot()      â”‚
â”‚                      â”‚                       â”‚              â”‚
â”‚                      â–¼                       â–¼              â”‚
â”‚               âŒ Bloqueado            âœ… Job Executa        â”‚
â”‚               se > 5 jobs            com slot reservado     â”‚
â”‚                                             â”‚              â”‚
â”‚                                             â–¼              â”‚
â”‚                                      releaseJobSlot()       â”‚
â”‚                                      ao completar           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Thresholds de ProteÃ§Ã£o

| Jobs Ativos | NÃ­vel | AÃ§Ã£o AutomÃ¡tica |
|-------------|-------|-----------------|
| 0-3 | ðŸŸ¢ Normal | Nenhuma |
| 4-5 | ðŸŸ¡ Elevated | Delay 2x |
| 6-8 | ðŸŸ  High | Delay 4x, alerta |
| 9+ | ðŸ”´ Critical | Kill Switch, cooldown 30min |

---

## Anexos

### A. Queries de DiagnÃ³stico Ãšteis

```sql
-- Jobs ativos por tipo
SELECT 
  'semantic' as type, status, COUNT(*) 
FROM semantic_annotation_jobs 
WHERE status IN ('processando', 'pendente')
GROUP BY status
UNION ALL
SELECT 
  'corpus' as type, status, COUNT(*) 
FROM corpus_annotation_jobs 
WHERE status IN ('processando', 'pendente')
GROUP BY status
UNION ALL
SELECT 
  'enrichment' as type, status, COUNT(*) 
FROM enrichment_jobs 
WHERE status IN ('processando', 'pendente')
GROUP BY status;
```

### B. ConfiguraÃ§Ã£o Redis Atual

```
UPSTASH_REDIS_REST_URL: [configurado]
UPSTASH_REDIS_REST_TOKEN: [configurado]

Keys utilizadas:
- emergency:kill_flag (TTL 30min)
- backpressure:cooldown_until (TTL variÃ¡vel)
- backpressure:trigger_reason (TTL variÃ¡vel)
- backpressure:metrics (TTL 5min)
- jobs:active_count (TTL 5min)
- jobs:slot:{job_id} (TTL 5min)
```

---

## AprovaÃ§Ãµes

| Papel | Nome | Data |
|-------|------|------|
| Autor | AI Assistant | 2024-12-12 |
| Revisor | [Pendente] | - |

---

*Documento criado como parte do processo de melhoria contÃ­nua do sistema Verso Austral.*
